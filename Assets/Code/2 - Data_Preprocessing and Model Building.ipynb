{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dddc90bc-36f1-489e-b714-e5188517f7d4",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "POJenyRlt0Xz",
    "outputId": "cb1f307d-9b6b-4911-b756-b1af38a43bff"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting pyspark\n  Using cached pyspark-3.5.1-py2.py3-none-any.whl\nRequirement already satisfied: py4j==0.10.9.7 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-46af17ae-d224-4ff0-a76f-ad0a1bd9665b/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\nInstalling collected packages: pyspark\nSuccessfully installed pyspark-3.5.1\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Data Preprocessing\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b6426cb-a941-45a6-8a59-26bb0d80d30b",
     "showTitle": false,
     "title": ""
    },
    "id": "-jRQq1yUORnw"
   },
   "source": [
    "### READING THE DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13ac3593-6786-4aeb-ba3a-74ea5d9ffa14",
     "showTitle": false,
     "title": ""
    },
    "id": "V7JK74-Xrqtb"
   },
   "outputs": [],
   "source": [
    "trump_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\",\"true\").option(\"quote\", \"\\\"\").option(\"escape\", \"\\\"\").option(\"multiline\", True).load(\"/mnt/2024-team19/hashtag_donaldtrump.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b98406eb-af7d-4b35-9b64-a8ec8c752027",
     "showTitle": false,
     "title": ""
    },
    "id": "MHDEHUCdrqtc"
   },
   "outputs": [],
   "source": [
    "biden_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\",\"true\").option(\"quote\", \"\\\"\").option(\"escape\", \"\\\"\").option(\"multiline\", True).load(\"/mnt/2024-team19/hashtag_joebiden.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6502adf5-590c-40c5-93b5-2ddded9a333c",
     "showTitle": false,
     "title": ""
    },
    "id": "LUfH8yirO3FN"
   },
   "source": [
    "### DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38e199dc-3aae-4422-b7a9-b2aa6e40e97a",
     "showTitle": false,
     "title": ""
    },
    "id": "guun8RZCQxPd"
   },
   "source": [
    "### 1. CREATING A NEW COLUMN HASHTAG FOR JOE BIDEN AND DONALD TRUMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ab16850-f766-4b22-818d-fbd476cd1087",
     "showTitle": false,
     "title": ""
    },
    "id": "koPi8SauQsh4"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "# Creating a new column named 'hashtag' with the value 'Trump'\n",
    "trump_df = trump_df.withColumn(\"hashtag\", lit(\"Trump\"))\n",
    "\n",
    "# Creating a new column named 'hashtag' with the value 'Biden'\n",
    "biden_df = biden_df.withColumn(\"hashtag\", lit(\"Biden\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "227bfcbe-20bf-4ca7-82c5-3fae75214c65",
     "showTitle": false,
     "title": ""
    },
    "id": "p7LcngMhQ2eu"
   },
   "source": [
    "### 2. MERGING BOTH THE DATAFRAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "185f1ea8-ad27-45c7-bee8-b29ac05209c7",
     "showTitle": false,
     "title": ""
    },
    "id": "PYmDUW9GQ2Sx"
   },
   "outputs": [],
   "source": [
    "combined_df = trump_df.union(biden_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a27d8c8-726c-4026-94e4-b28abdee6262",
     "showTitle": false,
     "title": ""
    },
    "id": "DQSG09-aQHdl"
   },
   "source": [
    "### 3. FILTERING THE DATA FOR UNITED STATES OF AMERICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f14ef6db-8448-4b3d-ab15-b5367b673cc5",
     "showTitle": false,
     "title": ""
    },
    "id": "LyTfR4eoQH9I"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "filtered_df = combined_df.filter((col(\"Country\") == \"United States of America\") | (col(\"Country\") == \"United States\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "781fae2c-d16c-4e0f-8ed3-649fee26dd1d",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4TB0LWGtrqtc",
    "outputId": "668189e5-682a-4323-f386-b8291b34602b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+-----+-------------+-------------------+--------------------+--------------------+----------------+--------------------+-------------------+--------------------+--------------------+------------------+-------------------+----------+--------------------+-------------+--------------------+----------+--------------------+-------+\n|         created_at|            tweet_id|               tweet|likes|retweet_count|             source|             user_id|           user_name|user_screen_name|    user_description|     user_join_date|user_followers_count|       user_location|               lat|               long|      city|             country|    continent|               state|state_code|        collected_at|hashtag|\n+-------------------+--------------------+--------------------+-----+-------------+-------------------+--------------------+--------------------+----------------+--------------------+-------------------+--------------------+--------------------+------------------+-------------------+----------+--------------------+-------------+--------------------+----------+--------------------+-------+\n|2020-10-15 00:00:01|1.316529221557252...|#Elecciones2020 |...|  0.0|          0.0|          TweetDeck|        3.60666534E8|  El Sol Latino News| elsollatinonews|üåê Noticias de in...|2011-08-23 15:33:45|              1860.0|Philadelphia, PA ...|          25.77427|          -80.19366|      NULL|United States of ...|North America|             Florida|        FL| 2020-10-21 00:00:00|  Trump|\n|2020-10-15 00:00:02|1.316529228091846...|#Trump: As a stud...|  2.0|          1.0|    Twitter Web App|           8436472.0|              snarke|          snarke|Will mock for foo...|2007-08-26 05:56:11|              1185.0|            Portland|        45.5202471|       -122.6741949|  Portland|United States of ...|North America|              Oregon|        OR|2020-10-21 00:00:...|  Trump|\n|2020-10-15 00:00:08|1.316529252301451...|You get a tie! An...|  4.0|          3.0| Twitter for iPhone|         4.7413798E7|Rana Abtar - ÿ±ŸÜÿß ...|       Ranaabtar|Washington Corres...|2009-06-15 19:05:35|              5393.0|       Washington DC|        38.8949924|        -77.0365581|Washington|United States of ...|North America|District of Columbia|        DC|2020-10-21 00:00:...|  Trump|\n|2020-10-15 00:00:17|1.316529291052675...|@CLady62 Her 15 m...|  2.0|          0.0|Twitter for Android|       1.138416104E9|        Farris Flagg|     FarrisFlagg|#BidenHarris2020 ...|2013-02-01 01:37:38|              2363.0|   Perris,California|        33.7825194|-117.22864779999999|      NULL|United States of ...|North America|          California|        CA|2020-10-21 00:00:...|  Trump|\n|2020-10-15 00:00:18|1.316529293497962...|@DeeviousDenise @...|  0.0|          0.0| Twitter for iPhone|9.007610716314296...|Stacey Gulledge ?...|     sm_gulledge|Patriot, Wife, ‚ÄúS...|2017-08-24 16:45:49|               766.0|           Ohio, USA|40.225356899999994|        -82.6881395|      NULL|United States of ...|North America|                Ohio|        OH|2020-10-21 00:00:...|  Trump|\n+-------------------+--------------------+--------------------+-----+-------------+-------------------+--------------------+--------------------+----------------+--------------------+-------------------+--------------------+--------------------+------------------+-------------------+----------+--------------------+-------------+--------------------+----------+--------------------+-------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "filtered_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59bc9eb5-49fe-4c1f-a8e6-4fd3d09fb7b6",
     "showTitle": false,
     "title": ""
    },
    "id": "tjXP-121T_ZA"
   },
   "source": [
    "### 4. DROPPING UNNECESSARY COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5da3011a-eb4e-419c-86b8-ab08997c6937",
     "showTitle": false,
     "title": ""
    },
    "id": "wVv3YMy4T_CI"
   },
   "outputs": [],
   "source": [
    "columns_to_drop =['created_at',\n",
    " 'tweet_id',\n",
    " 'retweet_count',\n",
    " 'likes',\n",
    " 'source',\n",
    " 'user_id',\n",
    " 'user_name',\n",
    " 'user_screen_name',\n",
    " 'user_description',\n",
    " 'user_join_date',\n",
    " 'user_followers_count',\n",
    " 'user_location',\n",
    " 'lat',\n",
    " 'long',\n",
    " 'city',\n",
    " 'country',\n",
    " 'continent',\n",
    " 'state',\n",
    " 'state_code',\n",
    " 'collected_at',\n",
    " 'sentiment_score',\n",
    " 'sentiment_label',\n",
    " ]\n",
    "\n",
    "# Drop the specified columns\n",
    "filtered_df = filtered_df.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b47b2a1-0c45-48cd-8e45-ae63c8bd29b8",
     "showTitle": false,
     "title": ""
    },
    "id": "GmzlDJ2_O7IO"
   },
   "source": [
    "#### 5. CREATING A SENTIMENT COLUMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0b217c1-cef5-4301-8cc3-bd64870fce16",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting vaderSentiment\n  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 126.0/126.0 kB 3.2 MB/s eta 0:00:00\nRequirement already satisfied: requests in /databricks/python3/lib/python3.10/site-packages (from vaderSentiment) (2.28.1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.10/site-packages (from requests->vaderSentiment) (1.26.14)\nRequirement already satisfied: charset-normalizer<3,>=2 in /databricks/python3/lib/python3.10/site-packages (from requests->vaderSentiment) (2.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.10/site-packages (from requests->vaderSentiment) (2022.12.7)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.10/site-packages (from requests->vaderSentiment) (3.4)\nInstalling collected packages: vaderSentiment\nSuccessfully installed vaderSentiment-3.3.2\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8268ef70-da28-4249-8d33-5b08a2716f78",
     "showTitle": false,
     "title": ""
    },
    "id": "xmcCH4XLrqtd"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "def calculate_sentiment(text):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    return sid.polarity_scores(text)['compound']\n",
    "\n",
    "# Register the function as a UDF (User Defined Function)\n",
    "sentiment_udf = udf(calculate_sentiment, DoubleType())\n",
    "\n",
    "# Apply the UDF to create a new column with sentiment scores\n",
    "filtered_df = filtered_df.withColumn(\"sentiment_score\", sentiment_udf(filtered_df[\"tweet\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f28518dc-1cef-473a-9c6c-c97b4a431cb0",
     "showTitle": false,
     "title": ""
    },
    "id": "nNpvLH5trqtd"
   },
   "outputs": [],
   "source": [
    "def score_to_sentiment_label(score):\n",
    "    if score > 0:\n",
    "        return \"positive\"\n",
    "    else:\n",
    "        return \"negative\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50d94c61-9374-4a62-b179-93af69c3e633",
     "showTitle": false,
     "title": ""
    },
    "id": "zXHJ0uZdTyFz"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Register the function as a UDF (User Defined Function)\n",
    "sentiment_label_udf = udf(score_to_sentiment_label, StringType())\n",
    "\n",
    "# Apply the UDF to create a new column with sentiment labels\n",
    "filtered_df = filtered_df.withColumn(\"sentiment_label\", sentiment_label_udf(filtered_df[\"sentiment_score\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48aef3ff-370d-4aeb-bd9c-e0002ac29fda",
     "showTitle": false,
     "title": ""
    },
    "id": "4YULp80mV-Io"
   },
   "outputs": [],
   "source": [
    "filtered_df = filtered_df.drop('sentiment_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c97f38c3-092c-445f-90bd-9676aa8f63df",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jvUmnzLCVHpy",
    "outputId": "93a67a9d-d68f-402d-f6dd-23f2f7dcf36c"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+---------------+\n|               tweet|hashtag|sentiment_label|\n+--------------------+-------+---------------+\n|#Elecciones2020 |...|  Trump|       negative|\n|#Trump: As a stud...|  Trump|       positive|\n|You get a tie! An...|  Trump|       negative|\n|@CLady62 Her 15 m...|  Trump|       negative|\n|@DeeviousDenise @...|  Trump|       negative|\n+--------------------+-------+---------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "filtered_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beb12dfd-50ce-4f00-9652-7fa3e28806e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "com.databricks.backend.common.rpc.CommandCancelledException\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:103)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$2(SequenceExecutionState.scala:103)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$2$adapted(SequenceExecutionState.scala:100)\n",
       "\tat scala.collection.immutable.Range.foreach(Range.scala:158)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:100)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:714)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:430)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:430)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.cancelExecution(ChauffeurState.scala:1225)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:958)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:573)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:67)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:67)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:67)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:573)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:542)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:67)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:915)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:672)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$4(Chauffeur.scala:696)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:696)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:749)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:554)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:573)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:573)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:542)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:147)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1020)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:941)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:545)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:514)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$4(ActivityContextFactory.scala:405)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:58)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$1(ActivityContextFactory.scala:405)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:380)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:159)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:514)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:404)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$4(InstrumentedQueuedThreadPool.scala:104)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:47)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:104)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:66)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:63)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:47)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:86)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.lang.Thread.run(Thread.java:750)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Cancelled"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "com.databricks.backend.common.rpc.CommandCancelledException",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:103)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$2(SequenceExecutionState.scala:103)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$2$adapted(SequenceExecutionState.scala:100)",
        "\tat scala.collection.immutable.Range.foreach(Range.scala:158)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:100)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:714)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:430)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:430)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.cancelExecution(ChauffeurState.scala:1225)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:958)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:573)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:67)",
        "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)",
        "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:67)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:67)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:573)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:542)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:67)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:915)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:672)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$4(Chauffeur.scala:696)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)",
        "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:696)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:749)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:554)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:573)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)",
        "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:573)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:542)",
        "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:147)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1020)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:941)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:545)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:514)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$4(ActivityContextFactory.scala:405)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:58)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$1(ActivityContextFactory.scala:405)",
        "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:380)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:159)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:514)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:404)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)",
        "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)",
        "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
        "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)",
        "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)",
        "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)",
        "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)",
        "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)",
        "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)",
        "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)",
        "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$4(InstrumentedQueuedThreadPool.scala:104)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:47)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:104)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:66)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:63)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:47)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:86)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)",
        "\tat java.lang.Thread.run(Thread.java:750)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, when, col\n",
    "\n",
    "# Aggregate to count positive and negative sentiments\n",
    "analysis_df = filtered_df.groupBy(\"hashtag\") \\\n",
    "    .agg(sum(when(col(\"sentiment_label\") == \"positive\", 1).otherwise(0)).alias(\"positive_count\"),\n",
    "         sum(when(col(\"sentiment_label\") == \"negative\", 1).otherwise(0)).alias(\"negative_count\"))\n",
    "\n",
    "# Show the analysis results\n",
    "analysis_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f1c3eaa-e3e5-4c40-8d99-eab7404a479a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+--------------+\n|hashtag|positive_count|negative_count|\n+-------+--------------+--------------+\n|  Trump|         77336|        135927|\n|  Biden|         75512|        105625|\n+-------+--------------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Register the DataFrame as a temporary view\n",
    "filtered_df.createOrReplaceTempView(\"tweets\")\n",
    "\n",
    "# Perform the analysis using SQL\n",
    "analysis_df = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        hashtag,\n",
    "        SUM(CASE WHEN sentiment_label = 'positive' THEN 1 ELSE 0 END) AS positive_count,\n",
    "        SUM(CASE WHEN sentiment_label = 'negative' THEN 1 ELSE 0 END) AS negative_count\n",
    "    FROM\n",
    "        tweets\n",
    "    WHERE\n",
    "        hashtag IN ('Trump', 'Biden')\n",
    "    GROUP BY\n",
    "        hashtag\n",
    "\"\"\")\n",
    "\n",
    "# Show the analysis results\n",
    "analysis_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "862a6693-c3d5-445a-9b78-a9ebbd7954ea",
     "showTitle": false,
     "title": ""
    },
    "id": "0tNKykPdWZTR"
   },
   "source": [
    "### TEXT CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4912b66-39ff-495e-9456-0328409862b4",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ckKXGUuSN2b_",
    "outputId": "819ad761-c8af-4c94-a9a3-cca822d23220"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting emoji\n  Downloading emoji-2.11.1-py2.py3-none-any.whl (433 kB)\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 433.8/433.8 kB 10.9 MB/s eta 0:00:00\nInstalling collected packages: emoji\nSuccessfully installed emoji-2.11.1\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "pip install emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "977bc87c-21d2-4e4e-861a-074cb5717c95",
     "showTitle": false,
     "title": ""
    },
    "id": "BjPoMdL7WhDx"
   },
   "source": [
    "### 1. Removing Hashtags, usernames and urls from Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd87c865-bbee-4b38-9246-169d948fdb41",
     "showTitle": false,
     "title": ""
    },
    "id": "a8qc_-idN2b_"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "\n",
    "def get_emoji_regexp():\n",
    "    # Sort emoji by length to make sure multi-character emojis are matched first\n",
    "    emojis = sorted(emoji.EMOJI_DATA, key=len, reverse=True)\n",
    "    pattern = '(' + '|'.join(re.escape(u) for u in emojis) + ')'\n",
    "    return re.compile(pattern)\n",
    "\n",
    "exp = get_emoji_regexp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd557190-185b-4922-b5fe-9b4d9d9920fe",
     "showTitle": false,
     "title": ""
    },
    "id": "hSG2Bh0QN2b_"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "# Define a function to remove URLs, emojis, and special characters\n",
    "def clean_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www.\\S+\", \"\", text)\n",
    "    # Remove emojis\n",
    "    text = re.sub(get_emoji_regexp(), \"\", text)\n",
    "    # Remove special characters\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "# Define a UDF for cleaning text\n",
    "clean_text_udf = udf(clean_text, StringType())\n",
    "\n",
    "# Apply cleaning to the \"tweet\" column\n",
    "processed_data = filtered_df.withColumn(\"tweet_cleaned\", clean_text_udf(\"tweet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a46922c-ffb7-4f91-9fd7-c9a73ab4af71",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qdc_OyjYN2b_",
    "outputId": "37a6c67f-3fb2-4ba0-fa2b-be81c3d8b11a"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+---------------+--------------------+\n|               tweet|hashtag|sentiment_label|       tweet_cleaned|\n+--------------------+-------+---------------+--------------------+\n|#Elecciones2020 |...|  Trump|       negative|Elecciones2020  E...|\n|#Trump: As a stud...|  Trump|       positive|Trump As a studen...|\n|You get a tie! An...|  Trump|       negative|You get a tie And...|\n|@CLady62 Her 15 m...|  Trump|       negative|CLady62 Her 15 mi...|\n|@DeeviousDenise @...|  Trump|       negative|DeeviousDenise re...|\n+--------------------+-------+---------------+--------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "processed_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ec39285-85d6-4576-b32c-217aae167d26",
     "showTitle": false,
     "title": ""
    },
    "id": "Z8cJJ1XmXl0n"
   },
   "source": [
    "### 2. TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ff7e773-d003-4fe0-956b-ed048b960379",
     "showTitle": false,
     "title": ""
    },
    "id": "3h4cGpMDrqte"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, HashingTF\n",
    "\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"tweet_cleaned\", outputCol=\"tokenized\", pattern=\"\\\\W\")\n",
    "tokenized = regexTokenizer.transform(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f11ad042-8c8a-4828-be34-715fc0858fee",
     "showTitle": false,
     "title": ""
    },
    "id": "CnxwzaCmrqtf"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"tokenized\", outputCol=\"cleaned\")\n",
    "sw_removed = remover.transform(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53f3e0e5-b2ee-48cb-8d2f-fc215e8f39a0",
     "showTitle": false,
     "title": ""
    },
    "id": "4OJXYO8crqtf"
   },
   "outputs": [],
   "source": [
    "hashTF = HashingTF(inputCol=\"cleaned\", outputCol=\"features\")\n",
    "result_df = hashTF.transform(sw_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ced649e-0d5e-47a3-892c-4186e527266f",
     "showTitle": false,
     "title": ""
    },
    "id": "wj5T1tcsrqtf"
   },
   "outputs": [],
   "source": [
    "columns_to_drop = ['tokenized', 'tweet', 'cleaned']\n",
    "result_df = result_df.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44f71e7e-98ea-4023-8c9f-41e875eee4f0",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mTrs0jE9rqtf",
    "outputId": "a606ef1a-69c0-4222-d291-f7e199ca10dd"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|hashtag|sentiment_label|tweet_cleaned                                                                                                                                                                                                                                                                                 |features                                                                                                                                                                                                                                                                                                  |\n+-------+---------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|Trump  |negative       |Elecciones2020  En Florida JoeBiden dice que DonaldTrump solo se preocupa por l mismo El demcrata fue anfitrin de encuentros de electores en PembrokePines y Miramar Clic AQU \\n\\n\\n\\nElSolLatino yobrilloconelsol                                                                            |(262144,[1303,30445,32297,43265,45835,46148,61392,72217,77899,92424,106834,112473,120751,124004,129701,131408,133494,142792,164359,168015,181864,183666,194559,201255,220451,252150,252367],[1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n|Trump  |positive       |Trump As a student I used to hear for years for ten years I heard China In 2019 And we have 15 and they dont know how many we have and I asked them how many do we have and they said sir we dont know But we have millions Like 300 million\\n\\nUm What                                       |(262144,[41129,55307,66273,76106,82288,87273,111370,120768,131217,136198,140931,153917,161061,168976,205538,208258,214720,230810,238047,245044,249283],[1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0])                                                             |\n|Trump  |negative       |You get a tie And you get a tie Trump s rally Iowa                                                                                                                                                                                                                                            |(262144,[46479,120768,229705,231821,252722],[2.0,1.0,1.0,1.0,2.0])                                                                                                                                                                                                                                        |\n|Trump  |negative       |CLady62 Her 15 minutes were over long time ago Omarosa never represented the black community TheReidOut \\n\\nShe cried to Trump begging for a job                                                                                                                                              |(262144,[12716,34366,36166,51832,66429,84145,99197,113673,120768,121517,129757,136198,146982,154828,187519,206312],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                     |\n|Trump  |negative       |DeeviousDenise realDonaldTrump nypost There wont be many of them  Unless you all have been voting more than once again  But God prevails  BO was the most corrupt President ever  Dark to light  Your lies are all coming through  They wouldnt last forever Trump                            |(262144,[5381,6801,12409,57915,109208,120768,137777,153178,154643,166714,194250,203802,205069,207834,228780,228929,229305,235273,241498,245044,247989],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                             |\n|Trump  |positive       |One of the single most effective remedies to eradicate another round of Trump Plague in our WhiteHouse                                                                                                                                                                                        |(262144,[1604,21823,58268,78833,83139,120768,134353,190003,251860,253382],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                                                                                      |\n|Trump  |negative       |In 2020 NYPost is being censorship CENSORED by Twitter to manipulate a US election in favor of JoeBiden and against Trump\\n\\nbut CCP from China or porn on Twitter \\n\\nThats always been fine for jack vijaya dickc KatieS\\n\\nmarciadorsey is jack sick                                       |(262144,[1004,1512,2406,10345,27286,33579,37834,58870,79779,87896,109156,120768,154435,163885,164055,182577,194559,217316,219529,228929,230810,245951,261677],[1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                              |\n|Trump  |positive       |Trump PresidentTrump Trump2020LandslideVictory Trump2020 MAGA KAG 4MoreYears America AmericaFirst AllLivesMatter Winning Vote VoteInPerson VoteTrump VotePresidentTrump                                                                                                                       |(262144,[22772,36149,49918,120768,136723,143603,163576,187873,219874,221520,230876,231338,258327,261333,261687],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                            |\n|Trump  |negative       |cnnbrk Trump owes RicardoAguirre 730000 to pay for the mass murder of his family TrumpLiedPeopleDied FauciFan                                                                                                                                                                                 |(262144,[51621,86807,92607,118384,120768,141368,199882,221017,232888,239713,252585],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                                                                        |\n|Trump  |positive       |Democrats have spent more tax payer paid time amp money on chasing amp attacking TRUMP not doing their jobs serving the public as elected USA NO GOOD REASONS NOT TO ECONOMICALLY BAIL PEOPLE OUT amp PROTECT THEM TOO AS YOU TOOK OATH TOO USA                                               |(262144,[5022,7019,68080,68723,80649,83895,91277,113432,117041,120768,121517,134711,149437,154459,174506,175778,176028,184523,185559,193809,235174,239982,240407,249943],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0])                               |\n|Trump  |positive       |Trump Nobody likes to tell you this but some of the farmers were doing better the way I was doing it than they were by working their asses off\\n\\nAnd that check Its totally in the mail right Don                                                                                            |(262144,[51471,65908,71450,85530,120768,191530,198131,200622,203005,203389,229166,229507,235375],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                                                   |\n|Trump  |negative       |RudyGiuliani Twitter PressSec You right RudyGiuliani Censorship should be Condemned \\nCc Trump and the GOP\\n\\nNot the American way\\n\\nNow lets redo Trumps Impeachment this time without the CENSORSHIP\\nAmericaFirst BidenCares DonaldTrump BidenWillCrushCovid BidenHarris2020ToSaveAmerica |(262144,[1512,2482,22772,51471,58870,70359,72799,120768,121517,127866,138836,158433,168737,169008,183339,183666,216662,223102,229166,238931,239127,258969],[1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                     |\n|Trump  |negative       |Comments on this Do Democrats Understand how Ruthless China is  China HunterBiden JoeBiden BidenHarris BidenHarris2020 TrumpPence2020 Trump realDonaldTrump WTO coronavirus trade                                                                                                             |(262144,[60862,91277,102211,120768,137777,158425,178221,180003,194559,194695,220547,220806,225159,230810,245605],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0])                                                                                                                           |\n|Trump  |negative       |karatblood KazePlaysJC Grab realDonaldTrump by the balls amp chuck the bastard out the door onto PennsylvaniaAvenue amp form a line amp everybody gets to kick DonaldTrump in the nuts Please note 1 kick per person only BidenHarrisToSaveAmerica VoteBlueToSaveOurDemocracy                 |(262144,[12650,43157,44876,59426,80431,85285,86576,91028,92651,103430,110078,137777,138021,152049,152823,159464,166368,178518,183666,198358,211294,227594,233903,239982,260465],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0])                    |\n|Trump  |negative       |Ice Cube is teaming up to work with President Trump in developing The Platinum Plan 2020election AfricanAmericans BlackAmerican DiamondandSilk DonaldTrump featuredonWJLive politics race TWJReports USnews                                                                                   |(262144,[454,34343,48489,62916,66034,84809,86856,103473,110647,120768,130733,141063,153178,183666,188746,221900,227686,232427,249855],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                      |\n|Trump  |negative       |BlacksForTrump \\nBlackVoicesForTrump \\nBidenIsARacist \\nBlackFathersMatter \\nBlackVotersMatter \\nVote GOP down ticket amp give Trump somebody to work with \\nDemocratsAreDestroyingAmerica                                                                                                    |(262144,[34343,38552,49918,53454,59676,107367,120768,146888,152065,161530,182817,215474,239127,239982],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                                         |\n|Trump  |negative       |ChrisDJackson New respect here for Scaramucci \\n\\nThe vile hateful ugliness of Trumps LawAndOrder America                                                                                                                                                                                     |(262144,[19282,40542,82079,89833,103770,156514,165112,183296,230876,238931],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                                                                                    |\n|Trump  |negative       |Who does trump owe 420 million dollars to What are the terms Who is DonaldTrump beholden to WE DEMAND TO KNOW\\nTrumpIsANationalSecurityRisk \\nFollowTheMoney \\nTrumpTaxCheat \\nTrumpTaxFraud                                                                                                  |(262144,[4978,55307,62829,89121,120768,140931,164184,172482,180504,183375,183666,188041,209741,215054],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                                         |\n|Trump  |negative       |JimJordan DevinNunes MattGaetz JohnCornyn BillBarr DonaldTrump amp the rest who railed about it should be indicted amp go to jail for perpetuating a fraud against the American people over the perfectly legal unmasking of General Flynn\\nThey make me sick\\n\\nLockThemUp                   |(262144,[3924,6590,28165,41351,46402,71882,89717,91767,94638,102119,102388,113241,114429,115776,138836,148675,152657,164342,183666,185559,225308,235184,239982,261677],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0])                                 |\n|Trump  |negative       |TheWeek Trump in Penn I saved suburbia I got rid of a regulation that was a disaster and it was really unfair and its gotten a lot worse under Obama and Biden You damn well better vote for me Pennsylvania you better vote But he never did identify the regulation                         |(262144,[8522,40957,46197,49918,51011,71175,98142,113312,113673,114179,120768,153946,158845,159901,186925,187359,216372,225898,229264,235375,242239,244728,245599],[1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0])                                         |\n+-------+---------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "result_df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac45d3a5-196f-4f3c-bdee-fb12433c6bd5",
     "showTitle": false,
     "title": ""
    },
    "id": "TuDwgFAjZA3O"
   },
   "source": [
    "### CREATING A CUSTOM FUNCTION TO ENCODE HASHTAG COLUMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bf27bec-4059-4bf5-aa31-c1edda6c86b9",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FxTII7gUY4wV",
    "outputId": "8b5195ae-6cd5-4289-e46e-c1974e8f341c"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+--------------------+--------------------+---------------+\n|hashtag|sentiment_label|       tweet_cleaned|            features|hashtag_encoded|\n+-------+---------------+--------------------+--------------------+---------------+\n|  Trump|       negative|Elecciones2020  E...|(262144,[1303,304...|              1|\n|  Trump|       positive|Trump As a studen...|(262144,[41129,55...|              1|\n|  Trump|       negative|You get a tie And...|(262144,[46479,12...|              1|\n|  Trump|       negative|CLady62 Her 15 mi...|(262144,[12716,34...|              1|\n|  Trump|       negative|DeeviousDenise re...|(262144,[5381,680...|              1|\n|  Trump|       positive|One of the single...|(262144,[1604,218...|              1|\n|  Trump|       negative|In 2020 NYPost is...|(262144,[1004,151...|              1|\n|  Trump|       positive|Trump PresidentTr...|(262144,[22772,36...|              1|\n|  Trump|       negative|cnnbrk Trump owes...|(262144,[51621,86...|              1|\n|  Trump|       positive|Democrats have sp...|(262144,[5022,701...|              1|\n|  Trump|       positive|Trump Nobody like...|(262144,[51471,65...|              1|\n|  Trump|       negative|RudyGiuliani Twit...|(262144,[1512,248...|              1|\n|  Trump|       negative|Comments on this ...|(262144,[60862,91...|              1|\n|  Trump|       negative|karatblood KazePl...|(262144,[12650,43...|              1|\n|  Trump|       negative|Ice Cube is teami...|(262144,[454,3434...|              1|\n|  Trump|       negative|BlacksForTrump \\n...|(262144,[34343,38...|              1|\n|  Trump|       negative|ChrisDJackson New...|(262144,[19282,40...|              1|\n|  Trump|       negative|Who does trump ow...|(262144,[4978,553...|              1|\n|  Trump|       negative|JimJordan DevinNu...|(262144,[3924,659...|              1|\n|  Trump|       negative|TheWeek Trump in ...|(262144,[8522,409...|              1|\n+-------+---------------+--------------------+--------------------+---------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Define the conditions for encoding\n",
    "trump_condition = (col(\"hashtag\") == \"Trump\")\n",
    "biden_condition = (col(\"hashtag\") == \"Biden\")\n",
    "\n",
    "# Define the values to assign for each condition\n",
    "trump_value = 1  # You can choose any numeric value\n",
    "biden_value = 2  #\n",
    "\n",
    "# Apply the conditions and assign values using when function\n",
    "result_df = result_df.withColumn(\"hashtag_encoded\",\n",
    "                                    when(trump_condition, trump_value)\n",
    "                                    .when(biden_condition, biden_value)\n",
    "                                    .otherwise(0))  # Assign 0 for other cases\n",
    "\n",
    "# Show the DataFrame with the new encoded column\n",
    "result_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9119f99d-ce9b-40a6-b268-be87bb1c34ec",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HBAhbfQurqtf",
    "outputId": "fe8fb390-225b-4a9c-af7a-a0d107b0e513"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+--------------------+--------------------+---------------+-----------------+\n|hashtag|sentiment_label|       tweet_cleaned|            features|hashtag_encoded|sentiment_encoded|\n+-------+---------------+--------------------+--------------------+---------------+-----------------+\n|  Trump|       negative|Elecciones2020  E...|(262144,[1303,304...|              1|                0|\n|  Trump|       positive|Trump As a studen...|(262144,[41129,55...|              1|                1|\n|  Trump|       negative|You get a tie And...|(262144,[46479,12...|              1|                0|\n|  Trump|       negative|CLady62 Her 15 mi...|(262144,[12716,34...|              1|                0|\n|  Trump|       negative|DeeviousDenise re...|(262144,[5381,680...|              1|                0|\n+-------+---------------+--------------------+--------------------+---------------+-----------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Defining the conditions for encoding\n",
    "negative_condition = (col(\"sentiment_label\") == \"negative\")\n",
    "positive_condition = (col(\"sentiment_label\") == \"positive\")\n",
    "\n",
    "# Applying the conditions and assign values using when function\n",
    "result_df = result_df.withColumn(\"sentiment_encoded\",\n",
    "                                    when(negative_condition, 0)\n",
    "                                    .when(positive_condition,1))\n",
    "\n",
    "# Shoing the DataFrame with the new encoded column\n",
    "result_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60c36bfc-d9ee-4d99-8d86-46a8567942a1",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oDv6ofVSpfd1",
    "outputId": "05fe34b9-d6ee-4417-c18a-29793ba1a02d"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filter the data into separate DataFrames based on the hashtag\n",
    "trump_data = result_df.filter(col(\"hashtag\") == \"Trump\")\n",
    "biden_data = result_df.filter(col(\"hashtag\") == \"Biden\")\n",
    "\n",
    "# Sample 25,000 rows from each DataFrame\n",
    "sampled_trump_data = trump_data.sample(False, 25000 / trump_data.count(), seed=42)\n",
    "sampled_biden_data = biden_data.sample(False, 25000 / biden_data.count(), seed=42)\n",
    "\n",
    "# Union the sampled subsets together to create the final DataFrame\n",
    "final_subset_data = sampled_trump_data.union(sampled_biden_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28fb3d6a-a1d5-4899-a3f3-7dc28c507598",
     "showTitle": false,
     "title": ""
    },
    "id": "8_mJJJDabT-r"
   },
   "source": [
    "### BUILDING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f961121-af8d-4cb8-a313-29549faf7116",
     "showTitle": false,
     "title": ""
    },
    "id": "pDidhkuNN2cB"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "featureassembler = VectorAssembler(inputCols= [\"features\",\"hashtag_encoded\"], outputCol= \"Independent Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb68e067-cff3-4d57-b77c-3342c8d499cf",
     "showTitle": false,
     "title": ""
    },
    "id": "NFUsIU8CctfU"
   },
   "outputs": [],
   "source": [
    "output = featureassembler.transform(final_subset_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7e54373-2c52-45a6-869d-109432aa982f",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WkcyFoZ0c1mN",
    "outputId": "239ae152-7979-4e19-920d-509a2900ec28"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+--------------------+--------------------+---------------+-----------------+--------------------+\n|hashtag|sentiment_label|       tweet_cleaned|            features|hashtag_encoded|sentiment_encoded|Independent Features|\n+-------+---------------+--------------------+--------------------+---------------+-----------------+--------------------+\n|  Trump|       positive|Trump PresidentTr...|(262144,[22772,36...|              1|                1|(262145,[22772,36...|\n|  Trump|       negative|ChrisDJackson New...|(262144,[19282,40...|              1|                0|(262145,[19282,40...|\n|  Trump|       negative|JimJordan DevinNu...|(262144,[3924,659...|              1|                0|(262145,[3924,659...|\n|  Trump|       negative|realDonaldTrump U...|(262144,[12072,18...|              1|                0|(262145,[12072,18...|\n|  Trump|       positive|Trump has been ba...|(262144,[93197,95...|              1|                1|(262145,[93197,95...|\n+-------+---------------+--------------------+--------------------+---------------+-----------------+--------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "output.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69d25cac-0f05-4700-8ed0-0bfb902d3174",
     "showTitle": false,
     "title": ""
    },
    "id": "vajgVCvnc_Hn"
   },
   "outputs": [],
   "source": [
    "finalized_data = output.select(\"Independent Features\",\"sentiment_encoded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e6fbd8e-3e41-404c-8f29-230f90a89843",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UIVMesp3dD4o",
    "outputId": "e7ad719c-cc5f-4e3d-ceab-5ac6010f9fde"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n|Independent Features|sentiment_encoded|\n+--------------------+-----------------+\n|(262145,[22772,36...|                1|\n|(262145,[19282,40...|                0|\n|(262145,[3924,659...|                0|\n|(262145,[12072,18...|                0|\n|(262145,[93197,95...|                1|\n+--------------------+-----------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "finalized_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91654192-2902-41b2-9a49-074a7c1e3338",
     "showTitle": false,
     "title": ""
    },
    "id": "euS2Q4mvdD2c"
   },
   "outputs": [],
   "source": [
    "# train test split\n",
    "train_data, test_data = finalized_data.randomSplit([0.75,0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e87771de-f073-4af3-b3ed-24533575520b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4281566180901004>, line 4\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m col, expr\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Convert the complex column to a string representation\u001B[39;00m\n",
       "\u001B[0;32m----> 4\u001B[0m test_data_csv \u001B[38;5;241m=\u001B[39m test_data\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIndependent Features\u001B[39m\u001B[38;5;124m\"\u001B[39m, expr(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mto_json(`Independent Features`)\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
       "\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Specify the path where you want to save the CSV file\u001B[39;00m\n",
       "\u001B[1;32m      7\u001B[0m csv_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/Workspace/2023-24-Teams/Team19/test_data.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     50\u001B[0m     )\n",
       "\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:6249\u001B[0m, in \u001B[0;36mDataFrame.withColumn\u001B[0;34m(self, colName, col)\u001B[0m\n",
       "\u001B[1;32m   6244\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(col, Column):\n",
       "\u001B[1;32m   6245\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n",
       "\u001B[1;32m   6246\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_COLUMN\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m   6247\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcol\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(col)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n",
       "\u001B[1;32m   6248\u001B[0m     )\n",
       "\u001B[0;32m-> 6249\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwithColumn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcolName\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcol\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jc\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1356\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:230\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    226\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    227\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    228\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    229\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 230\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [DATATYPE_MISMATCH.INVALID_JSON_SCHEMA] Cannot resolve \"to_json(Independent Features)\" due to data type mismatch: Input schema \"STRUCT<type: TINYINT, size: INT, indices: ARRAY<INT>, values: ARRAY<DOUBLE>>\" must be a struct, an array or a map. SQLSTATE: 42K09; line 1 pos 0;\n",
       "'Project [to_json(Independent Features#609948, Some(Etc/UTC)) AS Independent Features#610806, sentiment_encoded#607996]\n",
       "+- Sample 0.75, 1.0, false, 6312251830817302018\n",
       "   +- Sort [Independent Features#609948 ASC NULLS FIRST, sentiment_encoded#607996 ASC NULLS FIRST], false\n",
       "      +- Project [Independent Features#609948, sentiment_encoded#607996]\n",
       "         +- Project [hashtag#603449, sentiment_label#604692, tweet_cleaned#606137, features#606905, hashtag_encoded#607552, sentiment_encoded#607996, UDF(struct(features, features#606905, hashtag_encoded_double_VectorAssembler_2e87ef460ffa, cast(hashtag_encoded#607552 as double))) AS Independent Features#609948]\n",
       "            +- Union false, false\n",
       "               :- Sample 0.0, 0.11722614799566732, false, 42\n",
       "               :  +- Filter (hashtag#603449 = Trump)\n",
       "               :     +- Project [hashtag#603449, sentiment_label#604692, tweet_cleaned#606137, features#606905, hashtag_encoded#607552, CASE WHEN (sentiment_label#604692 = negative) THEN 0 WHEN (sentiment_label#604692 = positive) THEN 1 END AS sentiment_encoded#607996]\n",
       "               :        +- Project [hashtag#603449, sentiment_label#604692, tweet_cleaned#606137, features#606905, CASE WHEN (hashtag#603449 = Trump) THEN 1 WHEN (hashtag#603449 = Biden) THEN 2 ELSE 0 END AS hashtag_encoded#607552]\n",
       "               :           +- Project [hashtag#603449, sentiment_label#604692, tweet_cleaned#606137, features#606905]\n",
       "               :              +- Project [tweet#603361, hashtag#603449, sentiment_label#604692, tweet_cleaned#606137, tokenized#606679, cleaned#606893, UDF(cleaned#606893) AS features#606905]\n",
       "               :                 +- Project [tweet#603361, hashtag#603449, sentiment_label#604692, tweet_cleaned#606137, tokenized#606679, UDF(tokenized#606679) AS cleaned#606893]\n",
       "               :                    +- Project [tweet#603361, hashtag#603449, sentiment_label#604692, tweet_cleaned#606137, UDF(tweet_cleaned#606137) AS tokenized#606679]\n",
       "               :                       +- Project [tweet#603361, hashtag#603449, sentiment_label#604692, clean_text(tweet#603361)#606136 AS tweet_cleaned#606137]\n",
       "               :                          +- Project [tweet#603361, hashtag#603449, sentiment_label#604692]\n",
       "               :                             +- Project [tweet#603361, hashtag#603449, sentiment_score#604586, score_to_sentiment_label(sentiment_score#604586)#604691 AS sentiment_label#604692]\n",
       "               :                                +- Project [tweet#603361, hashtag#603449, calculate_sentiment(tweet#603361)#604585 AS sentiment_score#604586]\n",
       "               :                                   +- Project [tweet#603361, hashtag#603449]\n",
       "               :                                      +- Filter ((Country#603375 = United States of America) OR (Country#603375 = United States))\n",
       "               :                                         +- Union false, false\n",
       "               :                                            :- Project [created_at#603359, tweet_id#603360, tweet#603361, likes#603362, retweet_count#603363, source#603364, user_id#603365, user_name#603366, user_screen_name#603367, user_description#603368, user_join_date#603369, user_followers_count#603370, user_location#603371, lat#603372, long#603373, city#603374, country#603375, continent#603376, state#603377, state_code#603378, collected_at#603379, Trump AS hashtag#603449]\n",
       "               :                                            :  +- Relation [created_at#603359,tweet_id#603360,tweet#603361,likes#603362,retweet_count#603363,source#603364,user_id#603365,user_name#603366,user_screen_name#603367,user_description#603368,user_join_date#603369,user_followers_count#603370,user_location#603371,lat#603372,long#603373,city#603374,country#603375,continent#603376,state#603377,state_code#603378,collected_at#603379] csv\n",
       "               :                                            +- Project [created_at#597152, tweet_id#597153, tweet#597154, likes#597155, retweet_count#597156, source#597157, user_id#597158, user_name#597159, user_screen_name#597160, user_description#597161, user_join_date#597162, user_followers_count#597163, user_location#597164, lat#597165, long#597166, city#597167, country#597168, continent#597169, state#597170, state_code#597171, collected_at#597172, Biden AS hashtag#603473]\n",
       "               :                                               +- Relation [created_at#597152,tweet_id#597153,tweet#597154,likes#597155,retweet_count#597156,source#597157,user_id#597158,user_name#597159,user_screen_name#597160,user_description#597161,user_join_date#597162,user_followers_count#597163,user_location#597164,lat#597165,long#597166,city#597167,country#597168,continent#597169,state#597170,state_code#597171,collected_at#597172] csv\n",
       "               +- Project [hashtag#603449 AS hashtag#609671, sentiment_label#604692 AS sentiment_label#609672, tweet_cleaned#606137 AS tweet_cleaned#609673, features#606905 AS features#609674, hashtag_encoded#607552 AS hashtag_encoded#609675, sentiment_encoded#607996 AS sentiment_encoded#609676]\n",
       "                  +- Sample 0.0, 0.1380170809939438, false, 42\n",
       "                     +- Filter (hashtag#603449 = Biden)\n",
       "                        +- Project [hashtag#603449, sentiment_label#604692, tweet_cleaned#606137, features#606905, hashtag_encoded#607552, CASE WHEN (sentiment_label#604692 = negative) THEN 0 WHEN (sentiment_label#604692 = positive) THEN 1 END AS sentiment_encoded#607996]\n",
       "                           +- Project [hashtag#603449, sentiment_label#604692, tweet_cleaned#606137, features#606905, CASE WHEN (hashtag#603449 = Trump) THEN 1 WHEN (hashtag#603449 = Biden) THEN 2 ELSE 0 END AS hashtag_encoded#607552]\n",
       "                              +- Project [hashtag#603449, sentiment_label#604692, tweet_cleaned#606137, features#606905]\n",
       "                                 +- Project [tweet#609631, hashtag#603449, sentiment_label#604692, tweet_cleaned#606137, tokenized#606679, cleaned#606893, UDF(cleaned#606893) AS features#606905]\n",
       "                                    +- Project [tweet#609631, hashtag#603449, sentiment_label#604692, tweet_cleaned#606137, tokenized#606679, UDF(tokenized#606679) AS cleaned#606893]\n",
       "                                       +- Project [tweet#609631, hashtag#603449, sentiment_label#604692, tweet_cleaned#606137, UDF(tweet_cleaned#606137) AS tokenized#606679]\n",
       "                                          +- Project [tweet#609631, hashtag#603449, sentiment_label#604692, clean_text(tweet#609631)#606136 AS tweet_cleaned#606137]\n",
       "                                             +- Project [tweet#609631, hashtag#603449, sentiment_label#604692]\n",
       "                                                +- Project [tweet#609631, hashtag#603449, sentiment_score#604586, score_to_sentiment_label(sentiment_score#604586)#604691 AS sentiment_label#604692]\n",
       "                                                   +- Project [tweet#609631, hashtag#603449, calculate_sentiment(tweet#609631)#604585 AS sentiment_score#604586]\n",
       "                                                      +- Project [tweet#609631, hashtag#603449]\n",
       "                                                         +- Filter ((Country#609645 = United States of America) OR (Country#609645 = United States))\n",
       "                                                            +- Union false, false\n",
       "                                                               :- Project [created_at#609629, tweet_id#609630, tweet#609631, likes#609632, retweet_count#609633, source#609634, user_id#609635, user_name#609636, user_screen_name#609637, user_description#609638, user_join_date#609639, user_followers_count#609640, user_location#609641, lat#609642, long#609643, city#609644, country#609645, continent#609646, state#609647, state_code#609648, collected_at#609649, Trump AS hashtag#603449]\n",
       "                                                               :  +- Relation [created_at#609629,tweet_id#609630,tweet#609631,likes#609632,retweet_count#609633,source#609634,user_id#609635,user_name#609636,user_screen_name#609637,user_description#609638,user_join_date#609639,user_followers_count#609640,user_location#609641,lat#609642,long#609643,city#609644,country#609645,continent#609646,state#609647,state_code#609648,collected_at#609649] csv\n",
       "                                                               +- Project [created_at#609650, tweet_id#609651, tweet#609652, likes#609653, retweet_count#609654, source#609655, user_id#609656, user_name#609657, user_screen_name#609658, user_description#609659, user_join_date#609660, user_followers_count#609661, user_location#609662, lat#609663, long#609664, city#609665, country#609666, continent#609667, state#609668, state_code#609669, collected_at#609670, Biden AS hashtag#603473]\n",
       "                                                                  +- Relation [created_at#609650,tweet_id#609651,tweet#609652,likes#609653,retweet_count#609654,source#609655,user_id#609656,user_name#609657,user_screen_name#609658,user_description#609659,user_join_date#609660,user_followers_count#609661,user_location#609662,lat#609663,long#609664,city#609665,country#609666,continent#609667,state#609668,state_code#609669,collected_at#609670] csv\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "[DATATYPE_MISMATCH.INVALID_JSON_SCHEMA] Cannot resolve \"to_json(Independent Features)\" due to data type mismatch: Input schema \"STRUCT<type: TINYINT, size: INT, indices: ARRAY<INT>, values: ARRAY<DOUBLE>>\" must be a struct, an array or a map. SQLSTATE: 42K09; line 1 pos 0;\n'Project [to_json(Independent Features#609948, Some(Etc/UTC)) AS Independent Features#610806, sentiment_encoded#607996]\n+- Sample 0.75, 1.0, false, 6312251830817302018\n   +- Sort [Independent Features#609948 ASC NULLS FIRST, sentiment_encoded#607996 ASC NULLS FIRST], false\n      +- Project [Independent Features#609948, sentiment_encoded#607996]\n         +- Project [hashtag#603449, sentiment_label#604692, tweet_cleaned#606137, features#606905, hashtag_encoded#607552, sentiment_encoded#607996, UDF(struct(features, features#606905, hashtag_encoded_double_VectorAssembler_2e87ef460ffa, cast(hashtag_encoded#607552 as double))) AS Independent Features#609948]\n            +- Union false, false\n               :- Sample 0.0, 0.11722614799566732, false, 42\n               :  +- Filter (hashtag#603449 = Trump)\n               :     +- Project [hashtag#603449, sentiment_label#604692, tweet_cleaned#606137, features#606905, hashtag_encoded#607552, CASE WHEN (sentiment_label#604692 = negative) THEN 0 WHEN (sentiment_label#604692 = positive) THEN 1 END AS sentiment_encoded#607996]\n               :        +- Project [hashtag#603449, sentiment_label#604692, tweet_cleaned#606137, features#606905, CASE WHEN (hashtag#603449 = Trump) THEN 1 WHEN (hashtag#603449 = Biden) THEN 2 ELSE 0 END AS hashtag_encoded#607552]\n               :           +- Project [hashtag#603449, sentiment_label#604692, tweet_cleaned#606137, features#606905]\n               :              +- Project [tweet#603361, hashtag#603449, sentiment_label#604692, tweet_cleaned#606137, tokenized#606679, cleaned#606893, UDF(cleaned#606893) AS features#606905]\n               :                 +- Project [tweet#603361, hashtag#603449, sentiment_label#604692, tweet_cleaned#606137, tokenized#606679, UDF(tokenized#606679) AS cleaned#606893]\n               :                    +- Project [tweet#603361, hashtag#603449, sentiment_label#604692, tweet_cleaned#606137, UDF(tweet_cleaned#606137) AS tokenized#606679]\n               :                       +- Project [tweet#603361, hashtag#603449, sentiment_label#604692, clean_text(tweet#603361)#606136 AS tweet_cleaned#606137]\n               :                          +- Project [tweet#603361, hashtag#603449, sentiment_label#604692]\n               :                             +- Project [tweet#603361, hashtag#603449, sentiment_score#604586, score_to_sentiment_label(sentiment_score#604586)#604691 AS sentiment_label#604692]\n               :                                +- Project [tweet#603361, hashtag#603449, calculate_sentiment(tweet#603361)#604585 AS sentiment_score#604586]\n               :                                   +- Project [tweet#603361, hashtag#603449]\n               :                                      +- Filter ((Country#603375 = United States of America) OR (Country#603375 = United States))\n               :                                         +- Union false, false\n               :                                            :- Project [created_at#603359, tweet_id#603360, tweet#603361, likes#603362, retweet_count#603363, source#603364, user_id#603365, user_name#603366, user_screen_name#603367, user_description#603368, user_join_date#603369, user_followers_count#603370, user_location#603371, lat#603372, long#603373, city#603374, country#603375, continent#603376, state#603377, state_code#603378, collected_at#603379, Trump AS hashtag#603449]\n               :                                            :  +- Relation [created_at#603359,tweet_id#603360,tweet#603361,likes#603362,retweet_count#603363,source#603364,user_id#603365,user_name#603366,user_screen_name#603367,user_description#603368,user_join_date#603369,user_followers_count#603370,user_location#603371,lat#603372,long#603373,city#603374,country#603375,continent#603376,state#603377,state_code#603378,collected_at#603379] csv\n               :                                            +- Project [created_at#597152, tweet_id#597153, tweet#597154, likes#597155, retweet_count#597156, source#597157, user_id#597158, user_name#597159, user_screen_name#597160, user_description#597161, user_join_date#597162, user_followers_count#597163, user_location#597164, lat#597165, long#597166, city#597167, country#597168, continent#597169, state#597170, state_code#597171, collected_at#597172, Biden AS hashtag#603473]\n               :                                               +- Relation [created_at#597152,tweet_id#597153,tweet#597154,likes#597155,retweet_count#597156,source#597157,user_id#597158,user_name#597159,user_screen_name#597160,user_description#597161,user_join_date#597162,user_followers_count#597163,user_location#597164,lat#597165,long#597166,city#597167,country#597168,continent#597169,state#597170,state_code#597171,collected_at#597172] csv\n               +- Project [hashtag#603449 AS hashtag#609671, sentiment_label#604692 AS sentiment_label#609672, tweet_cleaned#606137 AS tweet_cleaned#609673, features#606905 AS features#609674, hashtag_encoded#607552 AS hashtag_encoded#609675, sentiment_encoded#607996 AS sentiment_encoded#609676]\n                  +- Sample 0.0, 0.1380170809939438, false, 42\n                     +- Filter (hashtag#603449 = Biden)\n                        +- Project [hashtag#603449, sentiment_label#604692, tweet_cleaned#606137, features#606905, hashtag_encoded#607552, CASE WHEN (sentiment_label#604692 = negative) THEN 0 WHEN (sentiment_label#604692 = positive) THEN 1 END AS sentiment_encoded#607996]\n                           +- Project [hashtag#603449, sentiment_label#604692, tweet_cleaned#606137, features#606905, CASE WHEN (hashtag#603449 = Trump) THEN 1 WHEN (hashtag#603449 = Biden) THEN 2 ELSE 0 END AS hashtag_encoded#607552]\n                              +- Project [hashtag#603449, sentiment_label#604692, tweet_cleaned#606137, features#606905]\n                                 +- Project [tweet#609631, hashtag#603449, sentiment_label#604692, tweet_cleaned#606137, tokenized#606679, cleaned#606893, UDF(cleaned#606893) AS features#606905]\n                                    +- Project [tweet#609631, hashtag#603449, sentiment_label#604692, tweet_cleaned#606137, tokenized#606679, UDF(tokenized#606679) AS cleaned#606893]\n                                       +- Project [tweet#609631, hashtag#603449, sentiment_label#604692, tweet_cleaned#606137, UDF(tweet_cleaned#606137) AS tokenized#606679]\n                                          +- Project [tweet#609631, hashtag#603449, sentiment_label#604692, clean_text(tweet#609631)#606136 AS tweet_cleaned#606137]\n                                             +- Project [tweet#609631, hashtag#603449, sentiment_label#604692]\n                                                +- Project [tweet#609631, hashtag#603449, sentiment_score#604586, score_to_sentiment_label(sentiment_score#604586)#604691 AS sentiment_label#604692]\n                                                   +- Project [tweet#609631, hashtag#603449, calculate_sentiment(tweet#609631)#604585 AS sentiment_score#604586]\n                                                      +- Project [tweet#609631, hashtag#603449]\n                                                         +- Filter ((Country#609645 = United States of America) OR (Country#609645 = United States))\n                                                            +- Union false, false\n                                                               :- Project [created_at#609629, tweet_id#609630, tweet#609631, likes#609632, retweet_count#609633, source#609634, user_id#609635, user_name#609636, user_screen_name#609637, user_description#609638, user_join_date#609639, user_followers_count#609640, user_location#609641, lat#609642, long#609643, city#609644, country#609645, continent#609646, state#609647, state_code#609648, collected_at#609649, Trump AS hashtag#603449]\n                                                               :  +- Relation [created_at#609629,tweet_id#609630,tweet#609631,likes#609632,retweet_count#609633,source#609634,user_id#609635,user_name#609636,user_screen_name#609637,user_description#609638,user_join_date#609639,user_followers_count#609640,user_location#609641,lat#609642,long#609643,city#609644,country#609645,continent#609646,state#609647,state_code#609648,collected_at#609649] csv\n                                                               +- Project [created_at#609650, tweet_id#609651, tweet#609652, likes#609653, retweet_count#609654, source#609655, user_id#609656, user_name#609657, user_screen_name#609658, user_description#609659, user_join_date#609660, user_followers_count#609661, user_location#609662, lat#609663, long#609664, city#609665, country#609666, continent#609667, state#609668, state_code#609669, collected_at#609670, Biden AS hashtag#603473]\n                                                                  +- Relation [created_at#609650,tweet_id#609651,tweet#609652,likes#609653,retweet_count#609654,source#609655,user_id#609656,user_name#609657,user_screen_name#609658,user_description#609659,user_join_date#609660,user_followers_count#609661,user_location#609662,lat#609663,long#609664,city#609665,country#609666,continent#609667,state#609668,state_code#609669,collected_at#609670] csv\n"
       },
       "metadata": {
        "errorSummary": "[DATATYPE_MISMATCH.INVALID_JSON_SCHEMA] Cannot resolve \"to_json(Independent Features)\" due to data type mismatch: Input schema \"STRUCT<type: TINYINT, size: INT, indices: ARRAY<INT>, values: ARRAY<DOUBLE>>\" must be a struct, an array or a map. SQLSTATE: 42K09"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "DATATYPE_MISMATCH.INVALID_JSON_SCHEMA",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "sqlState": "42K09",
        "startIndex": 0,
        "stopIndex": 30
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-4281566180901004>, line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m col, expr\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Convert the complex column to a string representation\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m test_data_csv \u001B[38;5;241m=\u001B[39m test_data\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIndependent Features\u001B[39m\u001B[38;5;124m\"\u001B[39m, expr(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mto_json(`Independent Features`)\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Specify the path where you want to save the CSV file\u001B[39;00m\n\u001B[1;32m      7\u001B[0m csv_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/Workspace/2023-24-Teams/Team19/test_data.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     50\u001B[0m     )\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:6249\u001B[0m, in \u001B[0;36mDataFrame.withColumn\u001B[0;34m(self, colName, col)\u001B[0m\n\u001B[1;32m   6244\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(col, Column):\n\u001B[1;32m   6245\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[1;32m   6246\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_COLUMN\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   6247\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcol\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(col)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n\u001B[1;32m   6248\u001B[0m     )\n\u001B[0;32m-> 6249\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwithColumn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcolName\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcol\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jc\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1356\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:230\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    226\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    227\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    228\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 230\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
        "\u001B[0;31mAnalysisException\u001B[0m: [DATATYPE_MISMATCH.INVALID_JSON_SCHEMA] Cannot resolve \"to_json(Independent Features)\" due to data type mismatch: Input schema \"STRUCT<type: TINYINT, size: INT, indices: ARRAY<INT>, values: ARRAY<DOUBLE>>\" must be a struct, an array or a map. SQLSTATE: 42K09; line 1 pos 0;\n'Project [to_json(Independent Features#609948, Some(Etc/UTC)) AS Independent Features#610806, sentiment_encoded#607996]\n+- Sample 0.75, 1.0, false, 6312251830817302018\n   +- Sort [Independent Features#609948 ASC NULLS FIRST, sentiment_encoded#607996 ASC NULLS FIRST], false\n      +- Project [Independent Features#609948, sentiment_encoded#607996]\n         +- Project [hashtag#603449, sentiment_label#604692, tweet_cleaned#606137, features#606905, hashtag_encoded#607552, sentiment_encoded#607996, UDF(struct(features, features#606905, hashtag_encoded_double_VectorAssembler_2e87ef460ffa, cast(hashtag_encoded#607552 as double))) AS Independent Features#609948]\n            +- Union false, false\n               :- Sample 0.0, 0.11722614799566732, false, 42\n               :  +- Filter (hashtag#603449 = Trump)\n               :     +- Project [hashtag#603449, sentiment_label#604692, tweet_cleaned#606137, features#606905, hashtag_encoded#607552, CASE WHEN (sentiment_label#604692 = negative) THEN 0 WHEN (sentiment_label#604692 = positive) THEN 1 END AS sentiment_encoded#607996]\n               :        +- Project [hashtag#603449, sentiment_label#604692, tweet_cleaned#606137, features#606905, CASE WHEN (hashtag#603449 = Trump) THEN 1 WHEN (hashtag#603449 = Biden) THEN 2 ELSE 0 END AS hashtag_encoded#607552]\n               :           +- Project [hashtag#603449, sentiment_label#604692, tweet_cleaned#606137, features#606905]\n               :              +- Project [tweet#603361, hashtag#603449, sentiment_label#604692, tweet_cleaned#606137, tokenized#606679, cleaned#606893, UDF(cleaned#606893) AS features#606905]\n               :                 +- Project [tweet#603361, hashtag#603449, sentiment_label#604692, tweet_cleaned#606137, tokenized#606679, UDF(tokenized#606679) AS cleaned#606893]\n               :                    +- Project [tweet#603361, hashtag#603449, sentiment_label#604692, tweet_cleaned#606137, UDF(tweet_cleaned#606137) AS tokenized#606679]\n               :                       +- Project [tweet#603361, hashtag#603449, sentiment_label#604692, clean_text(tweet#603361)#606136 AS tweet_cleaned#606137]\n               :                          +- Project [tweet#603361, hashtag#603449, sentiment_label#604692]\n               :                             +- Project [tweet#603361, hashtag#603449, sentiment_score#604586, score_to_sentiment_label(sentiment_score#604586)#604691 AS sentiment_label#604692]\n               :                                +- Project [tweet#603361, hashtag#603449, calculate_sentiment(tweet#603361)#604585 AS sentiment_score#604586]\n               :                                   +- Project [tweet#603361, hashtag#603449]\n               :                                      +- Filter ((Country#603375 = United States of America) OR (Country#603375 = United States))\n               :                                         +- Union false, false\n               :                                            :- Project [created_at#603359, tweet_id#603360, tweet#603361, likes#603362, retweet_count#603363, source#603364, user_id#603365, user_name#603366, user_screen_name#603367, user_description#603368, user_join_date#603369, user_followers_count#603370, user_location#603371, lat#603372, long#603373, city#603374, country#603375, continent#603376, state#603377, state_code#603378, collected_at#603379, Trump AS hashtag#603449]\n               :                                            :  +- Relation [created_at#603359,tweet_id#603360,tweet#603361,likes#603362,retweet_count#603363,source#603364,user_id#603365,user_name#603366,user_screen_name#603367,user_description#603368,user_join_date#603369,user_followers_count#603370,user_location#603371,lat#603372,long#603373,city#603374,country#603375,continent#603376,state#603377,state_code#603378,collected_at#603379] csv\n               :                                            +- Project [created_at#597152, tweet_id#597153, tweet#597154, likes#597155, retweet_count#597156, source#597157, user_id#597158, user_name#597159, user_screen_name#597160, user_description#597161, user_join_date#597162, user_followers_count#597163, user_location#597164, lat#597165, long#597166, city#597167, country#597168, continent#597169, state#597170, state_code#597171, collected_at#597172, Biden AS hashtag#603473]\n               :                                               +- Relation [created_at#597152,tweet_id#597153,tweet#597154,likes#597155,retweet_count#597156,source#597157,user_id#597158,user_name#597159,user_screen_name#597160,user_description#597161,user_join_date#597162,user_followers_count#597163,user_location#597164,lat#597165,long#597166,city#597167,country#597168,continent#597169,state#597170,state_code#597171,collected_at#597172] csv\n               +- Project [hashtag#603449 AS hashtag#609671, sentiment_label#604692 AS sentiment_label#609672, tweet_cleaned#606137 AS tweet_cleaned#609673, features#606905 AS features#609674, hashtag_encoded#607552 AS hashtag_encoded#609675, sentiment_encoded#607996 AS sentiment_encoded#609676]\n                  +- Sample 0.0, 0.1380170809939438, false, 42\n                     +- Filter (hashtag#603449 = Biden)\n                        +- Project [hashtag#603449, sentiment_label#604692, tweet_cleaned#606137, features#606905, hashtag_encoded#607552, CASE WHEN (sentiment_label#604692 = negative) THEN 0 WHEN (sentiment_label#604692 = positive) THEN 1 END AS sentiment_encoded#607996]\n                           +- Project [hashtag#603449, sentiment_label#604692, tweet_cleaned#606137, features#606905, CASE WHEN (hashtag#603449 = Trump) THEN 1 WHEN (hashtag#603449 = Biden) THEN 2 ELSE 0 END AS hashtag_encoded#607552]\n                              +- Project [hashtag#603449, sentiment_label#604692, tweet_cleaned#606137, features#606905]\n                                 +- Project [tweet#609631, hashtag#603449, sentiment_label#604692, tweet_cleaned#606137, tokenized#606679, cleaned#606893, UDF(cleaned#606893) AS features#606905]\n                                    +- Project [tweet#609631, hashtag#603449, sentiment_label#604692, tweet_cleaned#606137, tokenized#606679, UDF(tokenized#606679) AS cleaned#606893]\n                                       +- Project [tweet#609631, hashtag#603449, sentiment_label#604692, tweet_cleaned#606137, UDF(tweet_cleaned#606137) AS tokenized#606679]\n                                          +- Project [tweet#609631, hashtag#603449, sentiment_label#604692, clean_text(tweet#609631)#606136 AS tweet_cleaned#606137]\n                                             +- Project [tweet#609631, hashtag#603449, sentiment_label#604692]\n                                                +- Project [tweet#609631, hashtag#603449, sentiment_score#604586, score_to_sentiment_label(sentiment_score#604586)#604691 AS sentiment_label#604692]\n                                                   +- Project [tweet#609631, hashtag#603449, calculate_sentiment(tweet#609631)#604585 AS sentiment_score#604586]\n                                                      +- Project [tweet#609631, hashtag#603449]\n                                                         +- Filter ((Country#609645 = United States of America) OR (Country#609645 = United States))\n                                                            +- Union false, false\n                                                               :- Project [created_at#609629, tweet_id#609630, tweet#609631, likes#609632, retweet_count#609633, source#609634, user_id#609635, user_name#609636, user_screen_name#609637, user_description#609638, user_join_date#609639, user_followers_count#609640, user_location#609641, lat#609642, long#609643, city#609644, country#609645, continent#609646, state#609647, state_code#609648, collected_at#609649, Trump AS hashtag#603449]\n                                                               :  +- Relation [created_at#609629,tweet_id#609630,tweet#609631,likes#609632,retweet_count#609633,source#609634,user_id#609635,user_name#609636,user_screen_name#609637,user_description#609638,user_join_date#609639,user_followers_count#609640,user_location#609641,lat#609642,long#609643,city#609644,country#609645,continent#609646,state#609647,state_code#609648,collected_at#609649] csv\n                                                               +- Project [created_at#609650, tweet_id#609651, tweet#609652, likes#609653, retweet_count#609654, source#609655, user_id#609656, user_name#609657, user_screen_name#609658, user_description#609659, user_join_date#609660, user_followers_count#609661, user_location#609662, lat#609663, long#609664, city#609665, country#609666, continent#609667, state#609668, state_code#609669, collected_at#609670, Biden AS hashtag#603473]\n                                                                  +- Relation [created_at#609650,tweet_id#609651,tweet#609652,likes#609653,retweet_count#609654,source#609655,user_id#609656,user_name#609657,user_screen_name#609658,user_description#609659,user_join_date#609660,user_followers_count#609661,user_location#609662,lat#609663,long#609664,city#609665,country#609666,continent#609667,state#609668,state_code#609669,collected_at#609670] csv\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "# Convert the complex column to a string representation\n",
    "test_data_csv = test_data.withColumn(\"Independent Features\", expr(\"to_json(`Independent Features`)\"))\n",
    "\n",
    "# Specify the path where you want to save the CSV file\n",
    "csv_path = \"/Workspace/2023-24-Teams/Team19/test_data.csv\"\n",
    "\n",
    "# Write the DataFrame to CSV\n",
    "test_data_csv.write.csv(csv_path, header=True, mode=\"overwrite\")\n",
    "\n",
    "# Display the path to the CSV file\n",
    "print(\"CSV file saved to:\", csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76ea4f8a-d493-4e80-bd78-9076b1fffbd5",
     "showTitle": false,
     "title": ""
    },
    "id": "LfHPYMiPqB-7"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Assuming you have already assembled the features and prepared the data\n",
    "\n",
    "# Define the logistic regression model\n",
    "log_reg = LogisticRegression(featuresCol=\"Independent Features\", labelCol=\"sentiment_encoded\",maxIter=10,regParam=0.01)\n",
    "\n",
    "# Fit the logistic regression model\n",
    "model = log_reg.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29bcbc9e-abd6-4ffb-8dac-e1450452568c",
     "showTitle": false,
     "title": ""
    },
    "id": "OBf0Px0OqSZn"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.8534435493071367\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test data\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"sentiment_encoded\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Logistic Regression Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29be2521-f533-4b86-815a-cf89eaac7e53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    " \n",
    "# Assuming you have already assembled the features and prepared the data\n",
    " \n",
    "# Define the Random Forest classifier\n",
    "rf = RandomForestClassifier(featuresCol=\"Independent Features\", labelCol=\"sentiment_encoded\")\n",
    " \n",
    "# Fit the Random Forest model\n",
    "rf_model = rf.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a02b183e-9606-461f-979d-21628360c4bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.612232472736824\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test data\n",
    "rf_predictions = rf_model.transform(test_data)\n",
    " \n",
    "# Evaluate the model using BinaryClassificationEvaluator\n",
    "rf_evaluator = BinaryClassificationEvaluator(labelCol=\"sentiment_encoded\")\n",
    " \n",
    "# Area Under the ROC Curve (AUC) is the default metric for BinaryClassificationEvaluator\n",
    "rf_accuracy = rf_evaluator.evaluate(rf_predictions)\n",
    " \n",
    "# Print the accuracy\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd46dc0f-75a2-49bd-86bb-2fc85a15b9a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC Accuracy: 0.8748350682149089\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    " \n",
    "# Define the Linear SVC classifier\n",
    "svc = LinearSVC(featuresCol=\"Independent Features\", labelCol=\"sentiment_encoded\")\n",
    " \n",
    "# Fit the SVC model\n",
    "svc_model = svc.fit(train_data)\n",
    "svc_predictions = svc_model.transform(test_data)\n",
    " \n",
    "# Evaluate the model using BinaryClassificationEvaluator\n",
    "svc_evaluator = BinaryClassificationEvaluator(labelCol=\"sentiment_encoded\")\n",
    " \n",
    "# Area Under the ROC Curve (AUC) is the default metric for BinaryClassificationEvaluator\n",
    "svc_accuracy = svc_evaluator.evaluate(svc_predictions)\n",
    " \n",
    "# Print the accuracy\n",
    "print(\"SVC Accuracy:\", svc_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0defc0a9-9ba0-442d-ae09-e9d45c1ab9a2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### RESULTS:\n",
    "1. Logistic Regression : 85%\n",
    "2. Random Forest Classifier : 61%\n",
    "3. Support Vector Machine Classifier : 87%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bba66807-c793-4d13-97d9-ce105c4790e8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Support Vector Classifier proved to be the most efficient model for Twitter Sentiment Analysis."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "2 - Data_Preprocessing and Model Building",
   "widgets": {}
  },
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
